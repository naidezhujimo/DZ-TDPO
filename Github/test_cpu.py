import torch
import sys
import os
import shutil
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM

# ç¡®ä¿èƒ½å¯¼å…¥ dz_tdpo
sys.path.append(os.getcwd())

from dz_tdpo.config import TDPODKLConfig
from dz_tdpo.model import TemporalCausalLM
from dz_tdpo.trainer import TDPODKLTrainer
from dz_tdpo.data.dataset import TemporalPreferenceDataset, TemporalPreferenceSample

def run_smoke_test():
    print("ğŸš€ å¼€å§‹ CPU å†’çƒŸæµ‹è¯• (Dry Run)...")
    
    # 1. å‡†å¤‡å‡ç¯å¢ƒ (ä¿®æ”¹ç‰ˆï¼šWindows å®‰å…¨é€»è¾‘)
    output_dir = "./tmp_test_output"
    
    # ä¸å¼ºåˆ¶åˆ é™¤ï¼Œè€Œæ˜¯å¦‚æœå­˜åœ¨å°±ç›´æ¥ç”¨ï¼Œæˆ–è€…æ¢ä¸ªåå­—
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    else:
        # å¦‚æœæ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼Œå°è¯•æ¸…ç©ºé‡Œé¢çš„æ–‡ä»¶ï¼Œè€Œä¸æ˜¯åˆ é™¤æ–‡ä»¶å¤¹æœ¬èº«
        # è¿™æ ·é¿å…äº†æ–‡ä»¶å¤¹é”çš„é—®é¢˜
        for filename in os.listdir(output_dir):
            file_path = os.path.join(output_dir, filename)
            try:
                if os.path.isfile(file_path) or os.path.islink(file_path):
                    os.unlink(file_path)
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)
            except Exception as e:
                print(f"âš ï¸ è­¦å‘Š: æ— æ³•æ¸…ç†æ—§æ–‡ä»¶ {file_path}, ä½†æˆ‘ä»¬å°†ç»§ç»­å°è¯•è¿è¡Œã€‚åŸå› : {e}")

    device = torch.device("cpu") # å¼ºåˆ¶ä½¿ç”¨ CPU
    
    # 2. ä½¿ç”¨å¾®å‹æ¨¡å‹ (gpt2) ä»£æ›¿ Phi-3ï¼Œé¿å…çˆ†æ˜¾å­˜
    print("ğŸ“¦ åˆå§‹åŒ–å¾®å‹æ¨¡å‹ (GPT-2)...")
    tiny_model_name = "gpt2" # éå¸¸å°ï¼Œä»»ä½•ç”µè„‘éƒ½èƒ½è·‘
    tokenizer = AutoTokenizer.from_pretrained(tiny_model_name)
    tokenizer.pad_token = tokenizer.eos_token
    # æ·»åŠ å¿…è¦çš„ special tokens
    tokenizer.add_special_tokens({'additional_special_tokens': ["<|user|>", "<|assistant|>", "<|end|>"]})
    
    config_base = AutoConfig.from_pretrained(tiny_model_name)
    base_model = AutoModelForCausalLM.from_config(config_base) # éšæœºåˆå§‹åŒ–ï¼Œä¸ä¸‹è½½æƒé‡ï¼Œæ›´å¿«
    base_model.resize_token_embeddings(len(tokenizer))
    
    ref_base = AutoModelForCausalLM.from_config(config_base)
    ref_base.resize_token_embeddings(len(tokenizer))
    
    # 3. åˆå§‹åŒ– DZ-TDPO ç»„ä»¶
    print("âš™ï¸ åˆå§‹åŒ– Config å’Œ Wrapper...")
    config = TDPODKLConfig(
        model_name=tiny_model_name,
        use_temporal_bias=True, # æµ‹è¯•æˆ‘ä»¬çš„æ ¸å¿ƒç»„ä»¶
        use_adaptive_tau=False, # å…³æ‰ adaptive ä»¥å…éœ€è¦ä¸‹è½½ SBERT
        loss_type="tdpo",
        max_context_length=128
    )
    
    policy_model = TemporalCausalLM(base_model, config, device)
    ref_model = TemporalCausalLM(ref_base, config, device)
    
    # 4. æ„é€ å‡æ•°æ® (Mock Data)
    print("ğŸ“ æ„é€ å‡æ•°æ®...")
    dummy_samples = []
    for i in range(4): # é€ 4æ¡æ•°æ®
        ctx_len = 20
        seq_len = 10
        # éšæœºç”Ÿæˆ token ID
        ctx_ids = torch.randint(0, len(tokenizer), (ctx_len,))
        # æ¨¡æ‹Ÿ dataset è¾“å‡ºç»“æ„
        dummy_samples.append(TemporalPreferenceSample(
            context_ids=ctx_ids,
            context_turns=[0, 10, 20],
            chosen_reply_ids=torch.randint(0, len(tokenizer), (seq_len,)),
            rejected_reply_ids=torch.randint(0, len(tokenizer), (seq_len,)),
            turn_id=2,
            total_turns=3
        ))
        
    train_dataset = TemporalPreferenceDataset(dummy_samples, tokenizer, config)
    
    # 5. åˆå§‹åŒ– Trainer
    print("ğŸƒ åˆå§‹åŒ– Trainer...")
    trainer = TDPODKLTrainer(
        policy_model=policy_model,
        ref_model=ref_model,
        tokenizer=tokenizer,
        config=config,
        train_dataset=train_dataset,
        val_dataset=None,
        device=device,
        output_dir=output_dir,
        gradient_accumulation_steps=1
    )
    
    # 6. å°è¯•è¿è¡Œ 1 ä¸ª step
    print("ğŸ”¥ å°è¯•è¿è¡Œè®­ç»ƒ Step...")
    try:
        # å–å‡ºä¸€ä¸ª batch
        dataloader = trainer.train_loader
        batch = next(iter(dataloader))
        
        # è¿è¡Œä¸€æ­¥è®­ç»ƒ
        metrics = trainer.train_step(batch)
        print(f"âœ… è®­ç»ƒæ­¥æˆåŠŸ! Loss: {metrics['loss']:.4f}")
        
        # å°è¯•ä¿å­˜
        trainer.save_checkpoint("test_ckpt.pt")
        print("âœ… ä¿å­˜æ£€æŸ¥ç‚¹æˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥! é”™è¯¯ä¿¡æ¯:\n{e}")
        import traceback
        traceback.print_exc()
        return

    # æ¸…ç†
    shutil.rmtree(output_dir)
    print("\nâœ¨ æ­å–œï¼ä»£ç ç»“æ„éªŒè¯é€šè¿‡ã€‚å¯ä»¥å‘å¸ƒåˆ° GitHubï¼")

if __name__ == "__main__":
    run_smoke_test()